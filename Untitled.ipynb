{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43fad8c-0ae2-4a26-83d2-885b7413f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43533474-4b1c-486f-be7c-6fde8d5cb321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /opt/conda/lib/python3.11/site-packages (1.6.14)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.11/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.11/site-packages (from kaggle) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from kaggle) (4.66.1)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.11/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.11/site-packages (from kaggle) (2.0.7)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.11/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.11/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.11/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->kaggle) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->kaggle) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a1014a-b87d-4011-83a1-460cf039793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import zipfile\n",
    "# Path to the downloaded dataset\n",
    "zip_file_path = 'archive (1).zip'\n",
    "extraction_dir = 'twitter_sentiment_data'\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extraction_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8608f7f-d7a1-402e-b476-8152593b84b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2401  Borderlands  Positive  \\\n",
      "0  2401  Borderlands  Positive   \n",
      "1  2401  Borderlands  Positive   \n",
      "2  2401  Borderlands  Positive   \n",
      "3  2401  Borderlands  Positive   \n",
      "4  2401  Borderlands  Positive   \n",
      "\n",
      "  im getting on borderlands and i will murder you all ,  \n",
      "0  I am coming to the borders and I will kill you...     \n",
      "1  im getting on borderlands and i will kill you ...     \n",
      "2  im coming on borderlands and i will murder you...     \n",
      "3  im getting on borderlands 2 and i will murder ...     \n",
      "4  im getting into borderlands and i can murder y...     \n",
      "               2401\n",
      "count  74681.000000\n",
      "mean    6432.640149\n",
      "std     3740.423819\n",
      "min        1.000000\n",
      "25%     3195.000000\n",
      "50%     6422.000000\n",
      "75%     9601.000000\n",
      "max    13200.000000\n",
      "2401                                                       0\n",
      "Borderlands                                                0\n",
      "Positive                                                   0\n",
      "im getting on borderlands and i will murder you all ,    686\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Load the dataset\n",
    "file_path = os.path.join(extraction_dir, 'twitter_training.csv')  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57e98e4f-0f8a-4873-b599-73d671081c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3364   Facebook Irrelevant  \\\n",
      "0   352     Amazon    Neutral   \n",
      "1  8312  Microsoft   Negative   \n",
      "2  4371      CS-GO   Negative   \n",
      "3  4433     Google    Neutral   \n",
      "4  6273       FIFA   Negative   \n",
      "\n",
      "  I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£  \n",
      "0  BBC News - Amazon boss Jeff Bezos rejects clai...                                                                                                                                                                                                  \n",
      "1  @Microsoft Why do I pay for WORD when it funct...                                                                                                                                                                                                  \n",
      "2  CSGO matchmaking is so full of closet hacking,...                                                                                                                                                                                                  \n",
      "3  Now the President is slapping Americans in the...                                                                                                                                                                                                  \n",
      "4  Hi @EAHelp Iâ€™ve had Madeleine McCann in my cel...                                                                                                                                                                                                  \n",
      "               3364\n",
      "count    999.000000\n",
      "mean    6435.159159\n",
      "std     3728.912226\n",
      "min        6.000000\n",
      "25%     3241.500000\n",
      "50%     6560.000000\n",
      "75%     9662.500000\n",
      "max    13197.000000\n",
      "3364                                                                                                                                                                                                                                                  0\n",
      "Facebook                                                                                                                                                                                                                                              0\n",
      "Irrelevant                                                                                                                                                                                                                                            0\n",
      "I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = os.path.join(extraction_dir, 'twitter_validation.csv')  # Update with the actual CSV file name\n",
    "df_v = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_v.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(df_v.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df_v.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ed8793a-5af4-4e93-b168-14a63df5ec35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3364                                                                                                                                                                                                                                                  0\n",
      "Facebook                                                                                                                                                                                                                                              0\n",
      "Irrelevant                                                                                                                                                                                                                                            0\n",
      "I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#handling the null values in training dataset.\n",
    "df = df.dropna()\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b846782-cc47-4b0d-816a-d477592df366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned training data:\n",
      "  im getting on borderlands and i will murder you all ,  \\\n",
      "0  I am coming to the borders and I will kill you...      \n",
      "1  im getting on borderlands and i will kill you ...      \n",
      "2  im coming on borderlands and i will murder you...      \n",
      "3  im getting on borderlands 2 and i will murder ...      \n",
      "4  im getting into borderlands and i can murder y...      \n",
      "\n",
      "                      CleanedText  \n",
      "0              coming border kill  \n",
      "1      im getting borderland kill  \n",
      "2     im coming borderland murder  \n",
      "3  im getting borderland 2 murder  \n",
      "4    im getting borderland murder  \n",
      "Cleaned validation data:\n",
      "  I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£  \\\n",
      "0  BBC News - Amazon boss Jeff Bezos rejects clai...                                                                                                                                                                                                   \n",
      "1  @Microsoft Why do I pay for WORD when it funct...                                                                                                                                                                                                   \n",
      "2  CSGO matchmaking is so full of closet hacking,...                                                                                                                                                                                                   \n",
      "3  Now the President is slapping Americans in the...                                                                                                                                                                                                   \n",
      "4  Hi @EAHelp Iâ€™ve had Madeleine McCann in my cel...                                                                                                                                                                                                   \n",
      "\n",
      "                                         CleanedText  \n",
      "0  bbc news amazon bos jeff bezos reject claim co...  \n",
      "1  microsoft pay word function poorly samsungus c...  \n",
      "2  csgo matchmaking full closet hacking truly awf...  \n",
      "3  president slapping american face really commit...  \n",
      "4  hi eahelp â€™ madeleine mccann cellar past 13 ye...  \n"
     ]
    }
   ],
   "source": [
    "#data preprocessing using NLTK\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Check if the input is not a string and convert to string if necessary\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stop words and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Assume the last column is the text column\n",
    "text_column_index = df.shape[1] - 1\n",
    "\n",
    "# Convert all entries in the text column to strings and fill NaNs with empty strings\n",
    "df.iloc[:, text_column_index] = df.iloc[:, text_column_index].fillna('').astype(str)\n",
    "df_v.iloc[:, text_column_index] = df_v.iloc[:, text_column_index].fillna('').astype(str)\n",
    "\n",
    "# Apply preprocessing to the training and validation data using column indices\n",
    "df['CleanedText'] = df.iloc[:, text_column_index].apply(preprocess_text)\n",
    "df_v['CleanedText'] = df_v.iloc[:, text_column_index].apply(preprocess_text)\n",
    "\n",
    "# Display the cleaned text\n",
    "print(\"Cleaned training data:\")\n",
    "print(df.iloc[:, [text_column_index, -1]].head())\n",
    "print(\"Cleaned validation data:\")\n",
    "print(df_v.iloc[:, [text_column_index, -1]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29955714-c28e-49a8-9920-bd68094197d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data feature matrix: (74681, 5000)\n",
      "Shape of the validation data feature matrix: (999, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the text data\n",
    "X_train = tfidf.fit_transform(df['CleanedText']).toarray()\n",
    "X_val = tfidf.transform(df_v['CleanedText']).toarray()\n",
    "\n",
    "# Display the shape of the feature matrix\n",
    "print(\"Shape of the training data feature matrix:\", X_train.shape)\n",
    "print(\"Shape of the validation data feature matrix:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11220e83-789c-423f-87e5-a091039dc0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: 'int' object is not subscriptable; perhaps you missed a comma?\n",
      "<>:11: SyntaxWarning: 'int' object is not subscriptable; perhaps you missed a comma?\n",
      "/tmp/ipykernel_145/4234867055.py:11: SyntaxWarning: 'int' object is not subscriptable; perhaps you missed a comma?\n",
      "  print(df.iloc[3[1]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m y_val \u001b[38;5;241m=\u001b[39m df_v\u001b[38;5;241m.\u001b[39miloc[:, target_column_index]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(target_column_index)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# # Initialize and train the logistic regression model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# model = LogisticRegression(max_iter=1000)  # Increase max_iter if convergence is not achieved\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# model.fit(X_train, y_train)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print('Classification Report:')\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(classification_report(y_val, y_pred))\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the target variable\n",
    "# Assume the sentiment labels are in the second to last column\n",
    "target_column_index = df.shape[1] - 2\n",
    "y_train = df.iloc[:, target_column_index].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "y_val = df_v.iloc[:, target_column_index].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "print(target_column_index)\n",
    "print(df.iloc[3])\n",
    "# # Initialize and train the logistic regression model\n",
    "# model = LogisticRegression(max_iter=1000)  # Increase max_iter if convergence is not achieved\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the validation set\n",
    "# y_pred = model.predict(X_val)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print('Accuracy:', accuracy_score(y_val, y_pred))\n",
    "# print('Classification Report:')\n",
    "# print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc29d9-89f3-423e-aff0-7a930f3f2112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
